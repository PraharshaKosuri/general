{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dbadd31-6e63-46f1-a1a4-33101019801c",
   "metadata": {},
   "source": [
    "<h2><br>WDB Systems India Pvt Ltd</br>\n",
    "<br>Member 1:- KOSURI PRAHARSHA</br>\n",
    "<br>Member 2:- KOSURI VENU MADHAVA RAO </br>\n",
    "<br>SPECTRA PURE ‚ÄìMODEL</br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76f30fa-bbe7-4342-82d3-f2363b12278d",
   "metadata": {},
   "source": [
    "<h2> Import Libraries </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec6ea2ac-4abb-425c-9030-442db4573c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import scipy.optimize as op\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307abff5-fd6d-472e-bdad-8b2ee187db1a",
   "metadata": {},
   "source": [
    "<h2>Configuration </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77803472-b7a2-41d2-b857-4b9851b81ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file=\"heart_data.csv\"\n",
    "split_percentage=0.3\n",
    "\n",
    "#Model Tuning Paramaeters\n",
    "#alpha = 0.005\n",
    "alpha = 0.01\n",
    "#epochs = 150000\n",
    "epochs = 15000\n",
    "batch_size= 16\n",
    "lambda_reg = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "294cc61a-ec8c-41d0-b524-0d2b40ee0ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hrt_df = pd.read_csv(input_file)\n",
    "hrt_df.head(0)\n",
    "hrt_df.shape \n",
    "\n",
    "# Drop duplicates\n",
    "hrt_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Remove outliers using IQR (only for continuous features)\n",
    "col_num = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "for col in col_num:\n",
    "    Q1 = hrt_df[col].quantile(0.25)\n",
    "    Q3 = hrt_df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    LowerBound = Q1 - 1.5 * IQR\n",
    "    UpperBound = Q3 + 1.5 * IQR\n",
    "    hrt_df = hrt_df[(hrt_df[col] >= LowerBound) & (hrt_df[col] <= UpperBound)]\n",
    "\n",
    "hrt_df = hrt_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290af6fc-b687-4f4b-aed5-9cd2825d3c47",
   "metadata": {},
   "source": [
    "<h2>Extract input (X) and output (Y)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41409908-b8a7-4935-aeea-36f08f2b79d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am here\n",
      "[ 63.    1.    3.  145.  233.    1.    0.  150.    0.    2.3   0.    0.\n",
      "   1. ]\n",
      "‚úÖ Data Shape: (283, 13)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split features and target\n",
    "X = hrt_df.drop(columns=['target']).values\n",
    "print('i am here')\n",
    "print(X[0]);\n",
    "Y = hrt_df['target'].values.reshape(-1, 1) \n",
    "print(f\"‚úÖ Data Shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc0780-b48a-4141-9ca3-6b9c561a776d",
   "metadata": {},
   "source": [
    "<h2>Splitting training and test data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5de8216a-1f91-4389-ab5e-82c7d0d5d49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Ready. Train: (198, 13), Test: (85, 13)\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=split_percentage, random_state=20)\n",
    "print(f\"‚úÖ Data Ready. Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "'''\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train = poly.fit_transform(X_train)\n",
    "X_test = poly.transform(X_test)\n",
    "'''\n",
    "scaler = MinMaxScaler()#It is normalisation of features.\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "Y_train = Y_train.reshape(-1,1)\n",
    "Y_test = Y_test.reshape(-1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc5bd3d1-ea1a-4509-8384-031bf8317e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_new = X_test[-1].reshape(1, -1)\n",
    "Y_new = Y_test[-1]\n",
    "\n",
    "X_test = X_test[:-1]\n",
    "Y_test = Y_test[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12af0150-807f-46bb-a7e2-03555f18aaf8",
   "metadata": {},
   "source": [
    "<h2>Helper Functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "128b0789-4610-4334-88f2-d57218484b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_loss(y, y_pred):\n",
    "    m = len(y)\n",
    "    epsilon = 1e-8\n",
    "    return -(1/m) * np.sum(y*np.log(y_pred + epsilon) + (1-y)*np.log(1 - y_pred + epsilon))\n",
    "\n",
    "def f1_score_func(y_true, y_pred, threshold=0.5):\n",
    "    y_pred_class = (y_pred >= threshold).astype(int)\n",
    "    tp = np.sum((y_true == 1) & (y_pred_class == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred_class == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred_class == 0))\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    return f1\n",
    "\n",
    "def plot_loss(loss_history, title):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def evaluate_model(weights, bias, X_test, Y_test, title):\n",
    "    y_pred_probs = sigmoid(np.dot(X_test, weights) + bias)\n",
    "    y_pred_class = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(Y_test, y_pred_class)\n",
    "    report = classification_report(Y_test, y_pred_class, output_dict=True, zero_division=0)\n",
    "    conf_mat = confusion_matrix(Y_test, y_pred_class)\n",
    "    f1 = f1_score_func(Y_test, y_pred_probs)\n",
    "\n",
    "    tn, fp, fn, tp = conf_mat.ravel()\n",
    "    specificity = tn / (tn + fp + 1e-8)\n",
    "    sensitivity = tp / (tp + fn + 1e-8)\n",
    "\n",
    "    print(f\"\\nüìä {title} Results:\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1-Score:\", f1)\n",
    "    print(\"Sensitivity (Recall):\", sensitivity)\n",
    "    print(\"Specificity:\", specificity)\n",
    "    print(\"Confusion Matrix:\\n\", conf_mat)\n",
    "\n",
    "    return {\n",
    "        \"Method\": title,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Sensitivity\": sensitivity,\n",
    "        \"Specificity\": specificity\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0185fc0c-ac16-4c3c-bad5-00b099a85b17",
   "metadata": {},
   "source": [
    "<h2>Vanilla/Full Batch Gradient Descent</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f01a16ee-3ce5-4a1c-b039-8e4a3662e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent(X, Y, alpha, epochs):\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = np.zeros((n_features, 1))\n",
    "    bias = 0\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        z = np.dot(X, weights) + bias\n",
    "        y_pred = sigmoid(z)\n",
    "\n",
    "        dw = (1/n_samples) * np.dot(X.T, (y_pred - Y))\n",
    "        db = (1/n_samples) * np.sum(y_pred - Y)\n",
    "\n",
    "        weights -= alpha * dw\n",
    "        bias -= alpha * db\n",
    "\n",
    "        loss = compute_loss(Y, y_pred)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "    print(\"\\nüîπ Final Weights (Batch GD):\", weights.ravel())\n",
    "    print(\"üîπ Final Bias (Batch GD):\", bias)\n",
    "\n",
    "    return weights, bias, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72731135-c47a-4e06-a743-1a237ba0d6a9",
   "metadata": {},
   "source": [
    "<h2>Stoichastic Gradient Descent</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d02ed659-6db0-4cf7-80e3-56ad58512905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, Y, alpha, epochs):\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = np.zeros((n_features, 1))\n",
    "    bias = 0\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for i in indices:\n",
    "            x_i = X[i].reshape(1, -1)\n",
    "            y_i = Y[i]\n",
    "\n",
    "            z = np.dot(x_i, weights) + bias\n",
    "            y_pred = sigmoid(z)\n",
    "\n",
    "            dw = np.dot(x_i.T, (y_pred - y_i))\n",
    "            db = (y_pred - y_i)\n",
    "\n",
    "            weights -= alpha * dw\n",
    "            bias -= alpha * db\n",
    "\n",
    "        y_pred_all = sigmoid(np.dot(X, weights) + bias)\n",
    "        loss = compute_loss(Y, y_pred_all)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "    print(\"\\nüîπ Final Weights (Stochastic GD):\", weights.ravel())\n",
    "    print(\"üîπ Final Bias (Stochastic GD):\", bias)\n",
    "\n",
    "    return weights, bias, loss_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835447e7-3991-4882-a8a9-1e3041c11944",
   "metadata": {},
   "source": [
    "<h2>Minibatch Gradient Descent</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5457c5c-8bb8-4200-8c1a-76c49f474297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient_descent(X, Y, alpha, epochs, batch_size):\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = np.zeros((n_features, 1))\n",
    "    bias = 0\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        indices = np.arange(n_samples)\n",
    "        #print(\"indices: \",indices)\n",
    "        np.random.shuffle(indices)\n",
    "        X_s = X[indices]\n",
    "        Y_s = Y[indices]\n",
    "\n",
    "        for start in range(0, n_samples, batch_size):\n",
    "            end = start + batch_size\n",
    "            x_batch = X_s[start:end]\n",
    "            y_batch = Y_s[start:end]\n",
    "\n",
    "            z = np.dot(x_batch, weights) + bias\n",
    "            y_pred = sigmoid(z)\n",
    "\n",
    "            dw = (1 / len(x_batch)) * np.dot(x_batch.T, (y_pred - y_batch))\n",
    "            db = (1 / len(x_batch)) * np.sum(y_pred - y_batch)\n",
    "\n",
    "            weights -= alpha * dw\n",
    "            bias -= alpha * db\n",
    "\n",
    "        y_pred_all = sigmoid(np.dot(X, weights) + bias)\n",
    "        loss = compute_loss(Y, y_pred_all)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "    print(\"\\nüîπ Final Weights (Mini-Batch GD):\", weights.ravel())\n",
    "    print(\"üîπ Final Bias (Mini-Batch GD):\", bias)\n",
    "\n",
    "    return weights, bias, loss_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000385-779a-455f-bb82-2c79d2d581ab",
   "metadata": {},
   "source": [
    "<h2>Vanilla2 Gradient Descent</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d5df840-d962-460f-84cb-711d264303f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 5Ô∏è‚É£+ Vanilla Logistic Regression (using scipy.optimize)\n",
    "# --------------------------\n",
    "\n",
    "def vanilla2(X, Y):\n",
    "    import scipy.optimize as op\n",
    "\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def costFunction(theta, X, y):\n",
    "        m, n = X.shape\n",
    "        theta = theta.reshape((n, 1))\n",
    "        z = np.dot(X, theta)\n",
    "        h = sigmoid(z)\n",
    "        epsilon = 1e-8\n",
    "        cost = -(1/m) * np.sum(y*np.log(h+epsilon) + (1-y)*np.log(1-h+epsilon))\n",
    "        grad = (1/m) * np.dot(X.T, (h - y))\n",
    "        return cost, grad.flatten()\n",
    "\n",
    "    # Add bias column (intercept term)\n",
    "    X_bias = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    m, n = X_bias.shape\n",
    "    initial_theta = np.zeros((n, 1))\n",
    "\n",
    "    # Optimize using scipy fmin_tnc\n",
    "    result = op.fmin_tnc(func=costFunction, x0=initial_theta, args=(X_bias, Y))\n",
    "    theta_opt = result[0].reshape((n, 1))\n",
    "\n",
    "    weights = theta_opt[1:]\n",
    "    bias = theta_opt[0]\n",
    "\n",
    "    print(\"\\nüîπ Final Weights (Vanilla2):\", weights.ravel())\n",
    "    print(\"üîπ Final Bias (Vanilla2):\", bias)\n",
    "\n",
    "    return weights, bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095c343f-7be5-4efb-914d-a4431343e7b4",
   "metadata": {},
   "source": [
    "<h2>Training and Evaluating Models</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fff03d49-972e-4feb-8250-c6b17e54c805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Batch\\nw_b, b_b, loss_b = batch_gradient_descent(X_train, Y_train,alpha,epochs)\\nplot_loss(loss_b, \"Batch Gradient Descent Loss\")\\nresults.append(evaluate_model(w_b, b_b, X_test, Y_test, \"Batch Gradient Descent\"))\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "'''\n",
    "# Batch\n",
    "w_b, b_b, loss_b = batch_gradient_descent(X_train, Y_train,alpha,epochs)\n",
    "plot_loss(loss_b, \"Batch Gradient Descent Loss\")\n",
    "results.append(evaluate_model(w_b, b_b, X_test, Y_test, \"Batch Gradient Descent\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3bb1d34-d5c5-4e19-9c60-8eefac9c94d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Stochastic\\nw_s, b_s, loss_s = stochastic_gradient_descent(X_train, Y_train,alpha,epochs)\\nplot_loss(loss_s, \"Stochastic Gradient Descent Loss\")\\nresults.append(evaluate_model(w_s, b_s, X_test, Y_test, \"Stochastic Gradient Descent\"))\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Stochastic\n",
    "w_s, b_s, loss_s = stochastic_gradient_descent(X_train, Y_train,alpha,epochs)\n",
    "plot_loss(loss_s, \"Stochastic Gradient Descent Loss\")\n",
    "results.append(evaluate_model(w_s, b_s, X_test, Y_test, \"Stochastic Gradient Descent\"))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f0c76b0-a1f2-4528-a5f7-3542395a71cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Mini-Batch\\nw_m, b_m, loss_m = mini_batch_gradient_descent(X_train, Y_train,alpha,epochs,batch_size)\\nplot_loss(loss_m, \"Mini-Batch Gradient Descent Loss\")\\nresults.append(evaluate_model(w_m, b_m, X_test, Y_test, \"Mini-Batch Gradient Descent\"))\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Mini-Batch\n",
    "w_m, b_m, loss_m = mini_batch_gradient_descent(X_train, Y_train,alpha,epochs,batch_size)\n",
    "plot_loss(loss_m, \"Mini-Batch Gradient Descent Loss\")\n",
    "results.append(evaluate_model(w_m, b_m, X_test, Y_test, \"Mini-Batch Gradient Descent\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4f963ed-ee09-49bd-a4b6-6bafceb586e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Vanilla2 (using scipy.optimize)\\nw_v2, b_v2 = vanilla2(X_train, Y_train)\\nresults.append(evaluate_model(w_v2, b_v2, X_test, Y_test, \"Vanilla2 Logistic Regression\"))\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Vanilla2 (using scipy.optimize)\n",
    "w_v2, b_v2 = vanilla2(X_train, Y_train)\n",
    "results.append(evaluate_model(w_v2, b_v2, X_test, Y_test, \"Vanilla2 Logistic Regression\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a51a46e-737c-4e06-943c-43b7eb71e87d",
   "metadata": {},
   "source": [
    "<h2>Summary Table</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35b36a73-93f8-42c6-81f6-75b2f0045026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Final Performance Summary:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "summary_df = pd.DataFrame(results)\n",
    "print(\"\\n‚úÖ Final Performance Summary:\")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f722cb-8190-48e7-87b1-8340f179f3f1",
   "metadata": {},
   "source": [
    "<h2>Inference Cell</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22651f0e-5d6c-4bac-905c-588be0020b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Calling Vanilla Gradient Inference\")\\nY_output = inference(X_new, w_b, b_b)\\nprint(f\"Predicted value using vanilla gradient = {Y_output}, actual value = {Y_new}\")\\n#print(f\"X= {X_new}\",X_new)\\nprint(\"Calling Stoichastic Gradient Inference\")\\nY_output = inference(X_new, w_s, b_s)\\nprint(f\"Predicted value using stoichastic gradient = {Y_output}, actual value = {Y_new}\")\\n\\nprint(\"Calling Mini Gradient Inference\")\\nY_output = inference(X_new, w_m, b_m)\\nprint(f\"Predicted value using mini gradient = {Y_output}, actual value = {Y_new}\")\\n\\nprint(\"Calling Vanilla2 Gradient Inference\")\\nY_output = inference(X_new, w_v2, b_v2)\\nprint(f\"Predicted value using vanilla2 gradient = {Y_output}, actual value = {Y_new}\")\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inference(X_new, weights, bias):\n",
    "    y_predict = sigmoid(np.dot(X_new, weights) + bias)\n",
    "    #print(f\"Input values given = {X_new},\\nWeights = {weights},\\nBias = {bias}\")\n",
    "    if(y_predict>0.5):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "'''\n",
    "print(\"Calling Vanilla Gradient Inference\")\n",
    "Y_output = inference(X_new, w_b, b_b)\n",
    "print(f\"Predicted value using vanilla gradient = {Y_output}, actual value = {Y_new}\")\n",
    "#print(f\"X= {X_new}\",X_new)\n",
    "print(\"Calling Stoichastic Gradient Inference\")\n",
    "Y_output = inference(X_new, w_s, b_s)\n",
    "print(f\"Predicted value using stoichastic gradient = {Y_output}, actual value = {Y_new}\")\n",
    "\n",
    "print(\"Calling Mini Gradient Inference\")\n",
    "Y_output = inference(X_new, w_m, b_m)\n",
    "print(f\"Predicted value using mini gradient = {Y_output}, actual value = {Y_new}\")\n",
    "\n",
    "print(\"Calling Vanilla2 Gradient Inference\")\n",
    "Y_output = inference(X_new, w_v2, b_v2)\n",
    "print(f\"Predicted value using vanilla2 gradient = {Y_output}, actual value = {Y_new}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa23318-b679-41c4-b492-ed8abe983126",
   "metadata": {},
   "source": [
    "<h2>Built In Function(SVM)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0acf5c73-f18b-478e-b0e0-3e9f65dac1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.29787234 1.         0.         ... 0.5        0.         1.        ]\n",
      " [0.61702128 1.         0.66666667 ... 1.         0.5        1.        ]\n",
      " [0.59574468 1.         0.         ... 0.5        0.25       0.33333333]\n",
      " ...\n",
      " [0.4893617  1.         0.33333333 ... 1.         0.         0.66666667]\n",
      " [0.34042553 1.         0.         ... 0.5        0.         0.66666667]\n",
      " [0.46808511 1.         0.         ... 1.         0.         0.66666667]]\n",
      "[[-9.05726101e+00 -4.73206037e+00 -2.12506625e+00 -1.76718884e+00\n",
      "  -1.61204019e+01 -9.79340000e+00 -9.80632051e-01 -3.41999481e+01\n",
      "  -9.86009873e+01 -7.77842834e+00 -5.75035023e+01 -1.37476693e+00\n",
      "  -1.23119097e+00 -1.94504851e+01 -6.70668327e+01 -1.33556502e+01\n",
      "  -2.29155410e+01 -3.84955811e+00 -1.21669891e+01 -2.24577187e+01\n",
      "  -1.99771339e+01 -1.77725020e+01 -1.85682306e+01 -6.42029134e-01\n",
      "  -8.69245832e-01 -3.57302761e+00 -9.62968457e-01 -2.98265213e+01\n",
      "  -6.38482688e+00 -3.27326327e+01 -2.32158508e+01 -1.29424471e+01\n",
      "  -5.38200641e+00 -2.24820875e+01 -4.06759920e+00 -9.29271923e+00\n",
      "  -4.77230230e+01 -8.96188491e+00 -5.42864956e-01 -8.51801886e-01\n",
      "  -1.00000000e+02 -1.09329036e+01 -8.52005060e+00  2.18334068e+00\n",
      "   4.83901877e+00  2.19232442e+01  6.72332942e+01  1.43281799e+01\n",
      "   1.12499690e+00  3.71034474e+01  3.51964317e+00  1.30758879e+01\n",
      "   7.90844392e+00  1.05045054e+01  1.10920568e+00  1.76310096e+00\n",
      "   9.51824976e-01  2.30036744e+01  5.43804818e-01  1.61907414e+01\n",
      "   5.93428032e+01  6.39199624e-03  3.13855939e+01  1.01574361e+01\n",
      "   7.50694243e+00  6.12296660e+00  9.10988604e+00  6.61463905e+01\n",
      "   5.60203230e+00  6.68115873e+00  1.58173574e+01  3.06283882e+00\n",
      "   4.37878345e+01  4.12982252e-01  1.72342595e+00  3.33875101e+00\n",
      "   1.94291966e+01  4.33300579e+01  5.59240948e+00  3.58433285e+01\n",
      "   2.01154153e+00  1.62475081e+01  1.22417633e+00  1.01015089e+01\n",
      "   1.00000000e+02  1.44988329e+01  9.28328629e+00  1.03838573e+01\n",
      "   1.84025368e+01  8.89258002e+00]]\n",
      "[0.7816428]\n",
      "Predicted value using non linear svm= [0], actual value = [0]\n"
     ]
    }
   ],
   "source": [
    "def svm_linear(X, Y, lambda_reg=0.01): \n",
    "    # Ensure Y is 1D for sklearn\n",
    "    Y_sklearn = Y.ravel()  # converts (n,1) to (n,)  \n",
    "    model = LinearSVC(C=1/lambda_reg, max_iter=10000, random_state=20)      \n",
    "    model.fit(X, Y_sklearn)\n",
    "    return model.coef_.T,model.intercept_[0]\n",
    "  \n",
    "def svm_nonlinear(X, Y, kernel_type=\"linear\", lambda_reg=0.01, kernel_name=\"rbf\"):\n",
    "    Y_sklearn = Y.ravel()\n",
    "    model = SVC(kernel=kernel_name, C=1/lambda_reg, gamma='scale', random_state=20)\n",
    "    model.fit(X, Y_sklearn)\n",
    "    print(model.support_vectors_)\n",
    "    print(model.dual_coef_)   #Alpha_i*Y_i\n",
    "    print(model.intercept_)  #B\n",
    "    return model\n",
    "\n",
    "def svm_linear_inference(X,weights,bias):\n",
    "    result=np.dot(X,weights) + bias\n",
    "    print(result)\n",
    "    if (result>0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def svm_nonlinear_inference(X,model):\n",
    "    X=X.T\n",
    "    X=X.reshape(1,-1)\n",
    "    return model.predict(X)\n",
    "    \n",
    "'''\n",
    "weights_svm,bias_svm=svm_linear(X_train,Y_train,0.01)\n",
    "#print(f\"weights shape: {weights_svm.shape},bias shape: {bias_svm.shape}\",weights_svm.shape,bias_svm.shape)\n",
    "Y_output_svm=svm_linear_inference(X_new,weights_svm,bias_svm)\n",
    "print(f\"Predicted value using linear svm= {Y_output_svm}, actual value = {Y_new}\")\n",
    "X_new=X_test[0]\n",
    "Y_new=Y_test[0]\n",
    "print(X_new)\n",
    "print(Y_new)\n",
    "Y_output_svm=svm_linear_inference(X_new,weights_svm,bias_svm)\n",
    "print(f\"Predicted value using linear svm= {Y_output_svm}, actual value = {Y_new}\")\n",
    "'''\n",
    "\n",
    "svm_model_nonlinear=svm_nonlinear(X_train, Y_train, kernel_type=\"linear\", lambda_reg=0.01, kernel_name=\"rbf\")\n",
    "Y_output_svm=svm_nonlinear_inference(X_new,svm_model_nonlinear)\n",
    "print(f\"Predicted value using non linear svm= {Y_output_svm}, actual value = {Y_new}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0345ab67-9893-4bdc-9c84-2e66794a8a63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
